name: Deploy Backend to Oracle Cloud (Kubernetes)

on:
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - '.github/workflows/deploy-backend.yml'
  workflow_dispatch:

# Ensure only one deployment runs at a time
concurrency:
  group: backend-deployment
  cancel-in-progress: false

permissions:
  contents: read
  packages: write

jobs:
  deploy:
    runs-on: self-hosted  # Runs on your Oracle VM runner
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate image tag
        id: meta
        run: |
          export IMAGE_TAG=$(git rev-parse --short HEAD)
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          echo "Image tag: $IMAGE_TAG"

      - name: Build and push Colyseus image
        uses: docker/build-push-action@v5
        with:
          context: ./backend/colyseus
          push: true
          tags: ghcr.io/rishabhvenu/codeclashers-colyseus:${{ env.IMAGE_TAG }}
          cache-from: type=gha
          cache-to: type=gha

      - name: Build and push Bots image
        uses: docker/build-push-action@v5
        with:
          context: ./backend/bots
          push: true
          tags: ghcr.io/rishabhvenu/codeclashers-bots:${{ env.IMAGE_TAG }}
          cache-from: type=gha
          cache-to: type=gha

      - name: Build and push Judge0 ARM64 image
        run: |
          echo "Building Judge0 for ARM64..."
          
          # Login to GHCR to check if image exists
          echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          
          # Check if the latest image already exists
          if docker manifest inspect ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest &>/dev/null; then
            echo "‚úÖ Judge0 ARM64 image already exists in registry, skipping build"
            # Tag the existing image with the current commit
            docker pull ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest || true
            docker tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }}
            docker push ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }}
            echo "‚úÖ Tagged and pushed existing image with tag ${{ env.IMAGE_TAG }}"
          else
            echo "üîÑ Judge0 ARM64 image not found, building from source..."
            
            # Use existing builder or create one if missing
            docker buildx use arm64-builder || docker buildx create --name arm64-builder --use --bootstrap
            
            # Clone Judge0 repository if not exists
            if [ ! -d "judge0-build" ]; then
              git clone --depth 1 https://github.com/judge0/judge0.git judge0-build
            else
              cd judge0-build
              git pull
              cd ..
            fi
            
            cd judge0-build
            
            echo "Patching Dockerfile for ARM64 compatibility..."
            
            # Replace ALL base stages with Ubuntu 22.04
            sed -i 's|^FROM judge0/compilers:.*|FROM ubuntu:22.04 AS production|' Dockerfile
            sed -i 's|^FROM production.*|FROM ubuntu:22.04 AS development|' Dockerfile
            
            echo "Patched Dockerfile:"
            grep "^FROM" Dockerfile
            
            # Ensure scripts directory exists and workers script is present
            echo "Verifying scripts directory..."
            if [ ! -d "scripts" ]; then
              echo "‚ö†Ô∏è  scripts directory not found, checking repository structure..."
              find . -name "workers" -type f 2>/dev/null | head -5
              find . -type d -name "scripts" 2>/dev/null | head -5
            else
              echo "‚úì scripts directory found"
              ls -la scripts/ | head -10
              if [ ! -f "scripts/workers" ]; then
                echo "‚ö†Ô∏è  scripts/workers not found, checking for alternative location..."
                find . -name "workers" -type f 2>/dev/null
              else
                echo "‚úì scripts/workers found"
                chmod +x scripts/workers
              fi
            fi
            
            # Optional: limit languages for ARM64
            if command -v jq >/dev/null 2>&1; then
              jq 'del(.languages[] | select(.name | test("Swift|Pascal|Mono|C#|Go|Kotlin|PHP|Perl|Ruby"; "i")))' config.json > tmp.json && mv tmp.json config.json
            fi
            
            # Verify Dockerfile includes scripts before building
            echo "Checking Dockerfile for scripts copy commands..."
            if grep -q "COPY.*scripts" Dockerfile || grep -q "ADD.*scripts" Dockerfile; then
              echo "‚úì Dockerfile includes scripts copy command"
            else
              echo "‚ö†Ô∏è  Dockerfile may not copy scripts, checking Dockerfile structure..."
              grep -E "COPY|ADD" Dockerfile | grep -v "^#" | head -10
            fi
            
            # Build for ARM64 architecture using buildx
            docker buildx build \
              --platform linux/arm64 \
              --tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }} \
              --push .
            
            # Also tag as latest
            docker buildx build \
              --platform linux/arm64 \
              --tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest \
              --push .
            
            # Verify the built image has scripts/workers
            echo "Verifying built image contains scripts/workers..."
            docker pull ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest
            if docker run --rm --entrypoint sh ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest -c 'test -f ./scripts/workers && echo "‚úì scripts/workers exists" || echo "‚úó scripts/workers NOT FOUND"'; then
              echo "‚úÖ Image verification passed"
            else
              echo "‚ùå WARNING: Built image does not contain scripts/workers!"
              echo "   The image may not work correctly for workers deployment"
            fi
            
            cd ..
            echo "‚úÖ Judge0 ARM64 image built and pushed successfully"
          fi

      - name: Stop docker-compose services (if running)
        run: |
          # Stop docker-compose services to avoid port conflicts
          # This is safe - K8s will handle traffic
          if [ -f "/opt/CodeClashers/backend/docker-compose.prod.yml" ]; then
            echo "Stopping docker-compose services..."
            cd /opt/CodeClashers/backend
            docker-compose -f docker-compose.prod.yml down || true
            echo "Docker compose services stopped"
          else
            echo "No docker-compose.prod.yml found, skipping"
          fi

      - name: Install k3s if not present
        run: |
          if ! command -v k3s &> /dev/null; then
            echo "Installing k3s..."
            chmod +x backend/k8s/install-k3s.sh
            bash backend/k8s/install-k3s.sh
          else
            echo "k3s already installed"
            k3s --version
            
            # Fix permissions if kubeconfig is not readable
            if [ ! -r ~/.kube/config ]; then
              echo "Fixing kubeconfig permissions..."
              sudo mkdir -p ~/.kube
              sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
              sudo chown -R $USER:$USER ~/.kube
            fi
          fi
          
          # Ensure envsubst is installed (comes with gettext package)
          if ! command -v envsubst &> /dev/null; then
            echo "Installing gettext package for envsubst..."
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

      - name: Create namespace if not exists
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          k3s kubectl create namespace codeclashers --dry-run=client -o yaml | k3s kubectl apply -f -

      - name: Create registry secret for image pulls
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          k3s kubectl create secret docker-registry ghcr-secret \
            --namespace=codeclashers \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password="${{ secrets.GITHUB_TOKEN }}" \
            --dry-run=client -o yaml | k3s kubectl apply -f -

      - name: Create or update secrets
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          # Create MongoDB secrets (required by MongoDB StatefulSet)
          echo "Creating MongoDB secrets..."
          # Extract username and password from MONGODB_URI or use MONGODB_PASSWORD secret
          MONGODB_URI="${{ secrets.MONGODB_URI }}"
          # Try to extract from MONGODB_URI (format: mongodb://user:pass@host...)
          MONGO_USER=$(echo "$MONGODB_URI" | sed -n 's|mongodb://\([^:]*\):\([^@]*\)@.*|\1|p')
          MONGO_PASS=$(echo "$MONGODB_URI" | sed -n 's|mongodb://\([^:]*\):\([^@]*\)@.*|\2|p')
          
          # If extraction failed, use MONGODB_PASSWORD secret or fallback
          if [ -z "$MONGO_PASS" ]; then
            MONGO_PASS="${{ secrets.MONGODB_PASSWORD }}"
          fi
          if [ -z "$MONGO_USER" ]; then
            MONGO_USER="${{ secrets.MONGO_INITDB_ROOT_USERNAME || 'admin' }}"
          fi
          
          if [ -z "$MONGO_PASS" ]; then
            echo "‚ö†Ô∏è  Warning: Could not extract password from MONGODB_URI and MONGODB_PASSWORD secret not set."
          else
            echo "‚úì Using MongoDB user: $MONGO_USER"
          fi
          
          k3s kubectl create secret generic mongodb-secrets \
            --namespace=codeclashers \
            --from-literal=MONGO_INITDB_ROOT_USERNAME="$MONGO_USER" \
            --from-literal=MONGO_INITDB_ROOT_PASSWORD="$MONGO_PASS" \
            --from-literal=REPLICA_SET_NAME="rs0" \
            --dry-run=client -o yaml | k3s kubectl apply -f -
          
          # Create mongodb-keyfile if it doesn't exist (required for replica set authentication)
          if ! k3s kubectl get secret mongodb-keyfile -n codeclashers &>/dev/null; then
            echo "Creating mongodb-keyfile secret..."
            KEYFILE=$(openssl rand -base64 756)
            k3s kubectl create secret generic mongodb-keyfile \
              --namespace=codeclashers \
              --from-literal=keyfile="$KEYFILE"
            echo "‚úì mongodb-keyfile secret created"
          else
            echo "‚úì mongodb-keyfile secret already exists"
          fi
          
          # Create app-secrets with MongoDB URI including credentials
          # Use redis-cluster for internal access (not external domain)
          REDIS_HOST_INTERNAL="redis-cluster"
          MONGODB_URI_INTERNAL="mongodb://${MONGO_USER}:${MONGO_PASS}@mongodb.codeclashers.svc.cluster.local:${{ vars.MONGODB_PORT || '27017' }}/codeclashers?replicaSet=rs0&authSource=admin"
          
          k3s kubectl create secret generic app-secrets \
            --namespace=codeclashers \
            --from-literal=REDIS_PASSWORD="${{ secrets.REDIS_PASSWORD }}" \
            --from-literal=REDIS_HOST="$REDIS_HOST_INTERNAL" \
            --from-literal=REDIS_PORT="${{ vars.REDIS_PORT || '6379' }}" \
            --from-literal=JUDGE0_PORT="${{ vars.JUDGE0_PORT || '2358' }}" \
            --from-literal=MONGODB_PORT="${{ vars.MONGODB_PORT || '27017' }}" \
            --from-literal=COLYSEUS_PORT="${{ vars.COLYSEUS_PORT || '2567' }}" \
            --from-literal=JUDGE0_POSTGRES_USER="${{ secrets.JUDGE0_POSTGRES_USER }}" \
            --from-literal=JUDGE0_POSTGRES_PASSWORD="${{ secrets.JUDGE0_POSTGRES_PASSWORD }}" \
            --from-literal=JUDGE0_POSTGRES_DB="${{ secrets.JUDGE0_POSTGRES_DB }}" \
            --from-literal=MONGODB_URI_INTERNAL="$MONGODB_URI_INTERNAL" \
            --from-literal=MONGODB_URI="${{ secrets.MONGODB_URI }}" \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=INTERNAL_SERVICE_SECRET="${{ secrets.INTERNAL_SERVICE_SECRET }}" \
            --from-literal=BOT_SERVICE_SECRET="${{ secrets.BOT_SERVICE_SECRET }}" \
            --from-literal=COLYSEUS_RESERVATION_SECRET="${{ secrets.COLYSEUS_RESERVATION_SECRET }}" \
            --from-literal=AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}" \
            --from-literal=AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            --from-literal=S3_BUCKET_NAME="${{ vars.S3_BUCKET_NAME }}" \
            --from-literal=AWS_REGION="${{ vars.AWS_REGION }}" \
            --dry-run=client -o yaml | k3s kubectl apply -f -
          
          echo "‚úì All secrets created/updated"

      - name: Clean up old failing pods before deployment
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "Cleaning up old CrashLoopBackOff pods..."
          # Delete failed pods (includes CrashLoopBackOff and Error states)
          k3s kubectl delete pod -n codeclashers --field-selector=status.phase=Failed --grace-period=0 --force || true
          # Also delete any pods in CrashLoopBackOff or Error states
          k3s kubectl get pods -n codeclashers -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | while read pod; do
            status=$(k3s kubectl get pod -n codeclashers "$pod" -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}')
            if [[ "$status" == "CrashLoopBackOff" ]] || [[ "$status" == "Error" ]]; then
              echo "Deleting pod in $status state: $pod"
              k3s kubectl delete pod -n codeclashers "$pod" --grace-period=0 --force || true
            fi
          done
          sleep 3

      - name: Deploy to Kubernetes
        env:
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          KUBECONFIG: /home/ubuntu/.kube/config
          # MongoDB resources
          K8S_MONGODB_REPLICAS: ${{ secrets.K8S_MONGODB_REPLICAS || '1' }}
          K8S_MONGODB_MEMORY_REQUEST: ${{ secrets.K8S_MONGODB_MEMORY_REQUEST || '512Mi' }}
          K8S_MONGODB_MEMORY_LIMIT: ${{ secrets.K8S_MONGODB_MEMORY_LIMIT || '1Gi' }}
          K8S_MONGODB_CPU_REQUEST: ${{ secrets.K8S_MONGODB_CPU_REQUEST || '50m' }}
          K8S_MONGODB_CPU_LIMIT: ${{ secrets.K8S_MONGODB_CPU_LIMIT || '200m' }}
          # Redis resources
          K8S_REDIS_REPLICAS: ${{ secrets.K8S_REDIS_REPLICAS || '1' }}
          K8S_REDIS_MEMORY_REQUEST: ${{ secrets.K8S_REDIS_MEMORY_REQUEST || '256Mi' }}
          K8S_REDIS_MEMORY_LIMIT: ${{ secrets.K8S_REDIS_MEMORY_LIMIT || '512Mi' }}
          K8S_REDIS_CPU_REQUEST: ${{ secrets.K8S_REDIS_CPU_REQUEST || '50m' }}
          K8S_REDIS_CPU_LIMIT: ${{ secrets.K8S_REDIS_CPU_LIMIT || '200m' }}
          # PostgreSQL resources
          K8S_POSTGRES_REPLICAS: ${{ secrets.K8S_POSTGRES_REPLICAS || '1' }}
          K8S_POSTGRES_MEMORY_REQUEST: ${{ secrets.K8S_POSTGRES_MEMORY_REQUEST || '128Mi' }}
          K8S_POSTGRES_MEMORY_LIMIT: ${{ secrets.K8S_POSTGRES_MEMORY_LIMIT || '256Mi' }}
          K8S_POSTGRES_CPU_REQUEST: ${{ secrets.K8S_POSTGRES_CPU_REQUEST || '50m' }}
          K8S_POSTGRES_CPU_LIMIT: ${{ secrets.K8S_POSTGRES_CPU_LIMIT || '200m' }}
          # Colyseus resources
          K8S_COLYSEUS_REPLICAS: ${{ secrets.K8S_COLYSEUS_REPLICAS || '1' }}
          K8S_COLYSEUS_MEMORY_REQUEST: ${{ secrets.K8S_COLYSEUS_MEMORY_REQUEST || '256Mi' }}
          K8S_COLYSEUS_MEMORY_LIMIT: ${{ secrets.K8S_COLYSEUS_MEMORY_LIMIT || '512Mi' }}
          K8S_COLYSEUS_CPU_REQUEST: ${{ secrets.K8S_COLYSEUS_CPU_REQUEST || '50m' }}
          K8S_COLYSEUS_CPU_LIMIT: ${{ secrets.K8S_COLYSEUS_CPU_LIMIT || '200m' }}
          # Judge0 Server resources
          K8S_JUDGE0_SERVER_REPLICAS: ${{ secrets.K8S_JUDGE0_SERVER_REPLICAS || '1' }}
          K8S_JUDGE0_SERVER_MEMORY_REQUEST: ${{ secrets.K8S_JUDGE0_SERVER_MEMORY_REQUEST || '256Mi' }}
          K8S_JUDGE0_SERVER_MEMORY_LIMIT: ${{ secrets.K8S_JUDGE0_SERVER_MEMORY_LIMIT || '512Mi' }}
          K8S_JUDGE0_SERVER_CPU_REQUEST: ${{ secrets.K8S_JUDGE0_SERVER_CPU_REQUEST || '50m' }}
          K8S_JUDGE0_SERVER_CPU_LIMIT: ${{ secrets.K8S_JUDGE0_SERVER_CPU_LIMIT || '200m' }}
          # Judge0 Worker resources
          K8S_JUDGE0_WORKER_REPLICAS: ${{ secrets.K8S_JUDGE0_WORKER_REPLICAS || '1' }}
          K8S_JUDGE0_WORKER_MEMORY_REQUEST: ${{ secrets.K8S_JUDGE0_WORKER_MEMORY_REQUEST || '512Mi' }}
          K8S_JUDGE0_WORKER_MEMORY_LIMIT: ${{ secrets.K8S_JUDGE0_WORKER_MEMORY_LIMIT || '1Gi' }}
          K8S_JUDGE0_WORKER_CPU_REQUEST: ${{ secrets.K8S_JUDGE0_WORKER_CPU_REQUEST || '50m' }}
          K8S_JUDGE0_WORKER_CPU_LIMIT: ${{ secrets.K8S_JUDGE0_WORKER_CPU_LIMIT || '400m' }}
          # Bots resources
          K8S_BOTS_REPLICAS: ${{ secrets.K8S_BOTS_REPLICAS || '1' }}
          K8S_BOTS_MEMORY_REQUEST: ${{ secrets.K8S_BOTS_MEMORY_REQUEST || '128Mi' }}
          K8S_BOTS_MEMORY_LIMIT: ${{ secrets.K8S_BOTS_MEMORY_LIMIT || '256Mi' }}
          K8S_BOTS_CPU_REQUEST: ${{ secrets.K8S_BOTS_CPU_REQUEST || '50m' }}
          K8S_BOTS_CPU_LIMIT: ${{ secrets.K8S_BOTS_CPU_LIMIT || '200m' }}
        run: |
          set -e
          
          # Export image variables for envsubst
          export COLYSEUS_IMAGE="ghcr.io/rishabhvenu/codeclashers-colyseus:${IMAGE_TAG}"
          export BOTS_IMAGE="ghcr.io/rishabhvenu/codeclashers-bots:${IMAGE_TAG}"
          export JUDGE0_IMAGE="ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${IMAGE_TAG}"
          
          # Export secrets for envsubst (needed for StatefulSet init containers)
          # Note: These are only used for envsubst substitution, not exposed in final YAML
          export REDIS_PASSWORD="${{ secrets.REDIS_PASSWORD }}"
          
          # Apply manifests in order with environment substitution
          cd backend/k8s
          
          # Apply namespace
          k3s kubectl apply -f namespaces/
          
          # Apply ConfigMaps
          k3s kubectl apply -f configmaps/
          
          # Apply storage
          k3s kubectl apply -f storage/
          
          # Note: MongoDB StatefulSet deletion is handled in the MongoDB StatefulSet apply step
          # to avoid conflicts with multiple apply attempts
          
          # Apply MongoDB ConfigMap (required before StatefulSet)
          if [ -f mongodb/configmap.yaml ]; then
            echo "Applying MongoDB ConfigMap..."
            k3s kubectl apply -f mongodb/configmap.yaml
          fi
          
          # Apply MongoDB headless service (required for StatefulSet)
          if [ -f mongodb/headless-service.yaml ]; then
            echo "Applying MongoDB headless service..."
            k3s kubectl apply -f mongodb/headless-service.yaml
          fi
          
          # Apply MongoDB StatefulSet with env substitution
          if [ -f mongodb/statefulset.yaml ]; then
            echo "Applying MongoDB StatefulSet..."
            # Ensure K8S_MONGODB_REPLICAS is set and is an integer (default to 1 if not set)
            export K8S_MONGODB_REPLICAS="${K8S_MONGODB_REPLICAS:-1}"
            # Ensure it's a valid integer (remove any quotes or spaces)
            K8S_MONGODB_REPLICAS=$(echo "$K8S_MONGODB_REPLICAS" | tr -d '"' | tr -d "'" | tr -d ' ')
            export K8S_MONGODB_REPLICAS
            # Check if StatefulSet already exists - if so, delete it first (immutable fields)
            if k3s kubectl get statefulset mongodb -n codeclashers &>/dev/null; then
              echo "‚ö†Ô∏è  MongoDB StatefulSet exists, deleting before applying new spec..."
              k3s kubectl delete statefulset mongodb -n codeclashers --cascade=orphan --ignore-not-found=true
              k3s kubectl delete pods -n codeclashers -l app=mongodb --ignore-not-found=true || true
              sleep 5
            fi
            # Apply the StatefulSet
            envsubst < mongodb/statefulset.yaml | k3s kubectl apply -f -
          fi
          
          # Apply MongoDB PDB
          if [ -f mongodb/pdb.yaml ]; then
            echo "Applying MongoDB PodDisruptionBudget..."
            k3s kubectl apply -f mongodb/pdb.yaml
          fi
          
          # Services will be applied with env substitution below (after extracting VM IP)
          
          # Apply other statefulsets with env substitution (Redis, etc.)
          # Note: Delete old single-instance Redis if redis-cluster exists
          if [ -f statefulsets/redis-cluster.yaml ] && k3s kubectl get statefulset redis -n codeclashers &>/dev/null; then
            echo "‚ö†Ô∏è  Old single-instance Redis StatefulSet found. Deleting it to use redis-cluster..."
            k3s kubectl delete statefulset redis -n codeclashers --ignore-not-found=true
            echo "Waiting for old Redis pod to terminate..."
            sleep 3
          fi
          
          for file in statefulsets/*.yaml; do
            if [ -f "$file" ]; then
              # Skip MongoDB StatefulSet - it's already applied above
              if [[ "$(basename $file)" == *"mongodb"* ]]; then
                echo "Skipping $(basename $file) - MongoDB StatefulSet already applied"
                continue
              fi
              echo "Applying $(basename $file)..."
              # Check if this StatefulSet is MongoDB and delete it first if it exists
              STATEFULSET_NAME=$(grep -E "^name:|^  name:" "$file" | head -1 | sed 's/.*name: *//' | tr -d ' "')
              if [ "$STATEFULSET_NAME" = "mongodb" ]; then
                echo "‚ö†Ô∏è  Found MongoDB StatefulSet in $(basename $file), deleting existing one first..."
                k3s kubectl delete statefulset mongodb -n codeclashers --cascade=orphan --ignore-not-found=true || true
                k3s kubectl delete pods -n codeclashers -l app=mongodb --ignore-not-found=true || true
                sleep 3
              fi
              envsubst < "$file" | k3s kubectl apply -f -
            fi
          done
          
          # Wait briefly for StatefulSets to start pods
          echo "Waiting for StatefulSets to initialize..."
          sleep 3
          
          # Apply PodDisruptionBudgets before deployments
          k3s kubectl apply -f pdbs/
          
          # Get VM IP from COLYSEUS_HOST_IP variable
          export ORACLE_VM_IP="${{ vars.COLYSEUS_HOST_IP }}"
          
          if [ -z "$ORACLE_VM_IP" ]; then
            echo "‚ö†Ô∏è  Warning: COLYSEUS_HOST_IP not set. Services will use ClusterIP only."
            echo "   Set COLYSEUS_HOST_IP variable with your Oracle VM IP address"
          else
            echo "‚úì Using Oracle VM IP: $ORACLE_VM_IP"
            export ORACLE_VM_IP
          fi
          
          # Apply services with env substitution (for externalIPs)
          echo "Applying services..."
          for file in services/*.yaml; do
            if [ -f "$file" ]; then
              envsubst < "$file" | k3s kubectl apply -f -
            fi
          done
          
          # Apply MongoDB service if it exists
          if [ -f mongodb/service.yaml ]; then
            envsubst < mongodb/service.yaml | k3s kubectl apply -f -
          fi
          
          # Apply deployments in parallel for faster deployment
          # Apply all deployments first, then wait only for critical services
          echo "Applying all deployments..."
          for file in deployments/*.yaml; do
            if [ -f "$file" ]; then
              DEPLOYMENT_NAME=$(grep -E "^name:|^  name:" "$file" | head -1 | sed 's/.*name: *//' | tr -d ' "')
              echo "Applying deployment: $DEPLOYMENT_NAME"
              envsubst < "$file" | k3s kubectl apply -f -
            fi
          done
          
          echo "Deployments applied. Waiting for critical services..."
          
          # Wait only for critical service (colyseus) with shorter timeout
          # Other services continue in background
          if k3s kubectl wait --for=condition=available --timeout=120s deployment/colyseus -n codeclashers 2>/dev/null; then
            echo "‚úì Colyseus is ready"
          else
            echo "‚ö†Ô∏è  Colyseus not ready yet (will continue in background)"
          fi
          
          # Non-critical services: quick check, don't block
          for service in bots judge0-server judge0-worker; do
            if k3s kubectl get deployment $service -n codeclashers &>/dev/null; then
              if k3s kubectl wait --for=condition=available --timeout=30s deployment/$service -n codeclashers 2>/dev/null; then
                echo "‚úì $service is ready"
              else
                echo "‚ö†Ô∏è  $service still starting (non-critical, continuing in background)"
              fi
            fi
          done
          
          echo "Deployments applied. Rolling updates initiated."
          
          echo "All resources applied successfully"

      - name: Check pod status before rollout
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Current Pod Status ==="
          k3s kubectl get pods -n codeclashers
          echo ""
          echo "=== Pod Events (last 20) ==="
          k3s kubectl get events -n codeclashers --sort-by='.lastTimestamp' | tail -20
          echo ""
          echo "=== Node Status ==="
          k3s kubectl describe nodes | grep -A 5 "Allocatable:"
          echo ""
          echo "=== Unscheduleable Pods (if any) ==="
          k3s kubectl get pods -n codeclashers -o wide --field-selector=status.phase=Pending
          k3s kubectl describe pod -n codeclashers mongodb-0 | grep -A 5 Events || true

      - name: Wait for rollouts to complete
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "Checking deployment status..."
          
          # Quick non-blocking check - just wait for pods to exist
          sleep 3
          
          k3s kubectl get pods -n codeclashers
          
          echo "Deployments applied. Pods will continue starting in background."

      - name: Verify deployments
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Deployment Status ==="
          k3s kubectl get pods -n codeclashers
          echo ""
          echo "=== Services ==="
          k3s kubectl get svc -n codeclashers
          
      - name: Debug failed pods
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Debugging failed pods ==="
          
          # Check Judge0 server logs if it exists
          if k3s kubectl get deployment judge0-server -n codeclashers 2>/dev/null | grep -q judge0-server; then
            echo ""
            echo "--- Judge0 Server Logs ---"
            k3s kubectl logs -n codeclashers deployment/judge0-server --tail=50 || echo "No logs available yet"
          fi
          
          # Check Judge0 worker logs if it exists
          if k3s kubectl get deployment judge0-worker -n codeclashers 2>/dev/null | grep -q judge0-worker; then
            echo ""
            echo "--- Judge0 Worker Logs ---"
            k3s kubectl logs -n codeclashers deployment/judge0-worker --tail=50 || echo "No logs available yet"
          fi
          
          # Check MongoDB logs if it's having issues
          if k3s kubectl get pod mongodb-0 -n codeclashers 2>/dev/null | grep -v "1/1.*Running"; then
            echo ""
            echo "--- MongoDB Logs ---"
            k3s kubectl logs -n codeclashers mongodb-0 --tail=50 || echo "No logs available yet"
          fi
          
      - name: Pre-deployment validation
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Pre-deployment Validation ==="
          
          # Check if required secrets exist (check each one individually)
          echo "Checking required secrets..."
          MISSING_SECRETS=()
          
          # Check each secret individually
          if [ -z "${{ secrets.REDIS_PASSWORD }}" ]; then
            MISSING_SECRETS+=("REDIS_PASSWORD")
          fi
          if [ -z "${{ secrets.MONGODB_URI }}" ]; then
            MISSING_SECRETS+=("MONGODB_URI")
          fi
          # MONGODB_URI is required (password will be extracted from it)
          # MONGODB_PASSWORD is optional (fallback if MONGODB_URI doesn't contain password)
          if [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            MISSING_SECRETS+=("OPENAI_API_KEY")
          fi
          if [ -z "${{ secrets.INTERNAL_SERVICE_SECRET }}" ]; then
            MISSING_SECRETS+=("INTERNAL_SERVICE_SECRET")
          fi
          if [ -z "${{ secrets.BOT_SERVICE_SECRET }}" ]; then
            MISSING_SECRETS+=("BOT_SERVICE_SECRET")
          fi
          
          if [ ${#MISSING_SECRETS[@]} -gt 0 ]; then
            echo "‚ùå Missing required secrets: ${MISSING_SECRETS[*]}"
            exit 1
          fi
          
          echo "‚úì All required secrets present"
          
          # Validate image tags
          if [ -z "$IMAGE_TAG" ]; then
            echo "‚ùå IMAGE_TAG is not set"
            exit 1
          fi
          
          echo "‚úì Image tag: $IMAGE_TAG"
          
          # Check cluster connectivity
          if ! k3s kubectl cluster-info &>/dev/null; then
            echo "‚ùå Cannot connect to Kubernetes cluster"
            exit 1
          fi
          
          echo "‚úì Cluster connectivity verified"
          
      - name: Health check with rollback
        id: health_check
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Performing Enhanced Health Checks ==="
          
          HEALTH_CHECK_FAILED=false
          
          # Function to check HTTP endpoint
          check_http_endpoint() {
            local service=$1
            local port=$2
            local path=${3:-/}
            local max_attempts=${4:-30}
            
            echo "Checking $service HTTP endpoint (port $port, path $path)..."
            for i in $(seq 1 $max_attempts); do
              # Try to exec into pod and curl the endpoint
              if k3s kubectl exec -n codeclashers deployment/$service -- sh -c "curl -sf http://localhost:$port$path > /dev/null 2>&1" 2>/dev/null; then
                echo "‚úì $service HTTP endpoint is healthy"
                return 0
              fi
              if [ $i -lt $max_attempts ]; then
                sleep 2
              fi
            done
            echo "‚ùå $service HTTP endpoint health check failed"
            return 1
          }
          
          # Function to check port connectivity (optimized - fewer attempts)
          check_port() {
            local service=$1
            local port=$2
            local max_attempts=${3:-10}  # Reduced from 30 to 10
            
            echo "Checking $service port $port connectivity..."
            for i in $(seq 1 $max_attempts); do
              if k3s kubectl exec -n codeclashers deployment/$service -- sh -c "nc -z localhost $port" 2>/dev/null; then
                echo "‚úì $service port $port is open"
                return 0
              fi
              if [ $i -lt $max_attempts ]; then
                sleep 1  # Reduced from 2s to 1s
              fi
            done
            echo "‚ö†Ô∏è  $service port $port connectivity check failed (may still be starting)"
            return 1
          }
          
          # Wait for critical deployment (colyseus) with reduced timeout
          # Already waited in deployment step, but verify it's still healthy
          echo "Verifying critical services..."
          if k3s kubectl wait --for=condition=available --timeout=60s deployment/colyseus -n codeclashers 2>/dev/null; then
            echo "‚úì Colyseus is available"
          else
            echo "‚ö†Ô∏è  Colyseus health check timeout (may still be starting)"
            HEALTH_CHECK_FAILED=true
          fi
          
          # Bots deployment check (non-blocking - quick check only)
          if k3s kubectl get deployment bots -n codeclashers &>/dev/null; then
            if k3s kubectl wait --for=condition=available --timeout=30s deployment/bots -n codeclashers 2>/dev/null; then
              echo "‚úì Bots deployment is available"
            else
              echo "‚ö†Ô∏è  Bots deployment not ready yet (non-critical - continuing in background)"
            fi
          fi
          
          if [ "$HEALTH_CHECK_FAILED" = true ]; then
            echo "‚ùå Critical deployments failed to become available"
            echo "result=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check Colyseus port connectivity (reduced attempts)
          if ! check_port "colyseus" "2567" 10; then
            echo "‚ö†Ô∏è  Colyseus port check failed (may still be starting)"
            # Don't fail deployment - port may be accessible externally even if internal check fails
          fi
          
          # Check Colyseus HTTP endpoint (if available)
          # Note: Colyseus may not have a /health endpoint, so we check port only
          
          # Judge0 server already checked in deployment step above
          # Skip duplicate check here to save time
          
          # Check MongoDB if accessible
          if k3s kubectl get statefulset mongodb -n codeclashers &>/dev/null; then
            echo "Checking MongoDB connectivity..."
            for i in {1..30}; do
              if k3s kubectl exec -n codeclashers mongodb-0 -- mongosh --quiet --eval "db.adminCommand('ping')" 2>/dev/null | grep -q "ok.*1"; then
                echo "‚úì MongoDB is healthy"
                break
              fi
              if [ $i -eq 30 ]; then
                echo "‚ö†Ô∏è  MongoDB health check timeout (may still be initializing)"
              else
                sleep 2
              fi
            done
          fi
          
          if [ "$HEALTH_CHECK_FAILED" = true ]; then
            echo "‚ùå Health checks failed"
            echo "result=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "‚úì All health checks passed"
          echo "result=true" >> $GITHUB_OUTPUT
          
      - name: Rollback on health check failure
        if: steps.health_check.outputs.result == 'false'
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Rolling back deployment due to health check failure ==="
          
          # Get previous ReplicaSet
          PREVIOUS_RS=$(k3s kubectl get replicaset -n codeclashers -l app=colyseus --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-2].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$PREVIOUS_RS" ]; then
            echo "Rolling back Colyseus to previous ReplicaSet: $PREVIOUS_RS"
            k3s kubectl rollout undo deployment/colyseus -n codeclashers || true
          else
            echo "No previous ReplicaSet found for Colyseus"
          fi
          
          PREVIOUS_RS=$(k3s kubectl get replicaset -n codeclashers -l app=bots --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-2].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$PREVIOUS_RS" ]; then
            echo "Rolling back Bots to previous ReplicaSet: $PREVIOUS_RS"
            k3s kubectl rollout undo deployment/bots -n codeclashers || true
          else
            echo "No previous ReplicaSet found for Bots"
          fi
          
          echo "=== Rollback complete ==="
          echo "Please check pod logs and events for details:"
          k3s kubectl get pods -n codeclashers
          k3s kubectl get events -n codeclashers --sort-by='.lastTimestamp' | tail -20
          
          exit 1
          
      - name: Deployment complete
        if: steps.health_check.outputs.result == 'true'
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "üöÄ Deployment complete!"
          echo "Image tag: $IMAGE_TAG"
          echo ""
          echo "=== Service Status ==="
          k3s kubectl get svc -n codeclashers
          echo ""
          echo "‚úì Services are bound to Oracle VM IP with predefined ports:"
          echo "   - Colyseus: Port 2567"
          echo "   - Redis: Port 6379"
          echo "   - MongoDB: Port 27017"
          echo "   - Judge0: Port 2358"
          echo ""
          echo "‚úì No variable updates needed - using predefined IPs and ports from GitHub Variables"
