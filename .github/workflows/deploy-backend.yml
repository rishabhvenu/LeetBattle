name: Deploy Backend to Oracle Cloud (Kubernetes)

on:
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - '.github/workflows/deploy-backend.yml'
  workflow_dispatch:

# Ensure only one deployment runs at a time
concurrency:
  group: backend-deployment
  cancel-in-progress: false

permissions:
  contents: read
  packages: write

jobs:
  deploy:
    runs-on: self-hosted  # Runs on your Oracle VM runner
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate image tag
        id: meta
        run: |
          export IMAGE_TAG=$(git rev-parse --short HEAD)
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          echo "Image tag: $IMAGE_TAG"

      - name: Build and push Colyseus image
        uses: docker/build-push-action@v5
        with:
          context: ./backend/colyseus
          push: true
          tags: ghcr.io/rishabhvenu/codeclashers-colyseus:${{ env.IMAGE_TAG }}
          cache-from: type=gha
          cache-to: type=gha

      - name: Build and push Bots image
        uses: docker/build-push-action@v5
        with:
          context: ./backend/bots
          push: true
          tags: ghcr.io/rishabhvenu/codeclashers-bots:${{ env.IMAGE_TAG }}
          cache-from: type=gha
          cache-to: type=gha

      - name: Build and push Judge0 ARM64 image
        run: |
          echo "Building Judge0 for ARM64..."
          
          # Login to GHCR to check if image exists
          echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          
          # Check if the latest image already exists
          if docker manifest inspect ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest &>/dev/null; then
            echo "âœ… Judge0 ARM64 image already exists in registry, skipping build"
            # Tag the existing image with the current commit
            docker pull ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest || true
            docker tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }}
            docker push ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }}
            echo "âœ… Tagged and pushed existing image with tag ${{ env.IMAGE_TAG }}"
          else
            echo "ðŸ”„ Judge0 ARM64 image not found, building from source..."
            
            # Use existing builder or create one if missing
            docker buildx use arm64-builder || docker buildx create --name arm64-builder --use --bootstrap
            
            # Clone Judge0 repository if not exists
            if [ ! -d "judge0-build" ]; then
              git clone --depth 1 https://github.com/judge0/judge0.git judge0-build
            else
              cd judge0-build
              git pull
              cd ..
            fi
            
            cd judge0-build
            
            echo "Patching Dockerfile for ARM64 compatibility..."
            
            # Replace ALL base stages with Ubuntu 22.04
            sed -i 's|^FROM judge0/compilers:.*|FROM ubuntu:22.04 AS production|' Dockerfile
            sed -i 's|^FROM production.*|FROM ubuntu:22.04 AS development|' Dockerfile
            
            echo "Patched Dockerfile:"
            grep "^FROM" Dockerfile
            
            # Optional: limit languages for ARM64
            if command -v jq >/dev/null 2>&1; then
              jq 'del(.languages[] | select(.name | test("Swift|Pascal|Mono|C#|Go|Kotlin|PHP|Perl|Ruby"; "i")))' config.json > tmp.json && mv tmp.json config.json
            fi
            
            # Build for ARM64 architecture using buildx
            docker buildx build \
              --platform linux/arm64 \
              --tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:${{ env.IMAGE_TAG }} \
              --push .
            
            # Also tag as latest
            docker buildx build \
              --platform linux/arm64 \
              --tag ghcr.io/rishabhvenu/codeclashers-judge0-arm64:latest \
              --push .
            
            cd ..
            echo "âœ… Judge0 ARM64 image built and pushed successfully"
          fi

      - name: Stop docker-compose services (if running)
        run: |
          # Stop docker-compose services to avoid port conflicts
          # This is safe - K8s will handle traffic
          if [ -f "/opt/CodeClashers/backend/docker-compose.prod.yml" ]; then
            echo "Stopping docker-compose services..."
            cd /opt/CodeClashers/backend
            docker-compose -f docker-compose.prod.yml down || true
            echo "Docker compose services stopped"
          else
            echo "No docker-compose.prod.yml found, skipping"
          fi

      - name: Install k3s if not present
        run: |
          if ! command -v k3s &> /dev/null; then
            echo "Installing k3s..."
            chmod +x backend/k8s/install-k3s.sh
            bash backend/k8s/install-k3s.sh
          else
            echo "k3s already installed"
            k3s --version
            
            # Fix permissions if kubeconfig is not readable
            if [ ! -r ~/.kube/config ]; then
              echo "Fixing kubeconfig permissions..."
              sudo mkdir -p ~/.kube
              sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
              sudo chown -R $USER:$USER ~/.kube
            fi
          fi
          
          # Ensure envsubst is installed (comes with gettext package)
          if ! command -v envsubst &> /dev/null; then
            echo "Installing gettext package for envsubst..."
            sudo apt-get update && sudo apt-get install -y gettext-base
          fi

      - name: Create namespace if not exists
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          k3s kubectl create namespace codeclashers --dry-run=client -o yaml | k3s kubectl apply -f -

      - name: Create registry secret for image pulls
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          k3s kubectl create secret docker-registry ghcr-secret \
            --namespace=codeclashers \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password="${{ secrets.GITHUB_TOKEN }}" \
            --dry-run=client -o yaml | k3s kubectl apply -f -

      - name: Create or update secrets
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          k3s kubectl create secret generic app-secrets \
            --namespace=codeclashers \
            --from-literal=REDIS_PASSWORD="${{ secrets.REDIS_PASSWORD }}" \
            --from-literal=REDIS_HOST="${{ vars.REDIS_HOST || 'redis-cluster' }}" \
            --from-literal=REDIS_PORT="${{ vars.REDIS_PORT || '6379' }}" \
            --from-literal=JUDGE0_PORT="${{ vars.JUDGE0_PORT || '2358' }}" \
            --from-literal=MONGODB_PORT="${{ vars.MONGODB_PORT || '27017' }}" \
            --from-literal=COLYSEUS_PORT="${{ vars.COLYSEUS_PORT || '2567' }}" \
            --from-literal=JUDGE0_POSTGRES_USER="${{ secrets.JUDGE0_POSTGRES_USER }}" \
            --from-literal=JUDGE0_POSTGRES_PASSWORD="${{ secrets.JUDGE0_POSTGRES_PASSWORD }}" \
            --from-literal=JUDGE0_POSTGRES_DB="${{ secrets.JUDGE0_POSTGRES_DB }}" \
            --from-literal=MONGODB_URI_INTERNAL="mongodb://mongodb.codeclashers.svc.cluster.local:${{ vars.MONGODB_PORT || '27017' }}/codeclashers?replicaSet=rs0&authSource=admin" \
            --from-literal=MONGODB_URI="${{ secrets.MONGODB_URI }}" \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=INTERNAL_SERVICE_SECRET="${{ secrets.INTERNAL_SERVICE_SECRET }}" \
            --from-literal=BOT_SERVICE_SECRET="${{ secrets.BOT_SERVICE_SECRET }}" \
            --from-literal=COLYSEUS_RESERVATION_SECRET="${{ secrets.COLYSEUS_RESERVATION_SECRET }}" \
            --from-literal=AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}" \
            --from-literal=AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            --from-literal=S3_BUCKET_NAME="${{ vars.S3_BUCKET_NAME }}" \
            --from-literal=AWS_REGION="${{ vars.AWS_REGION }}" \
            --dry-run=client -o yaml | k3s kubectl apply -f -

      - name: Clean up old failing pods before deployment
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "Cleaning up old CrashLoopBackOff pods..."
          # Delete failed pods (includes CrashLoopBackOff and Error states)
          k3s kubectl delete pod -n codeclashers --field-selector=status.phase=Failed --grace-period=0 --force || true
          # Also delete any pods in CrashLoopBackOff or Error states
          k3s kubectl get pods -n codeclashers -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | while read pod; do
            status=$(k3s kubectl get pod -n codeclashers "$pod" -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}')
            if [[ "$status" == "CrashLoopBackOff" ]] || [[ "$status" == "Error" ]]; then
              echo "Deleting pod in $status state: $pod"
              k3s kubectl delete pod -n codeclashers "$pod" --grace-period=0 --force || true
            fi
          done
          sleep 5

      - name: Deploy to Kubernetes
        env:
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          KUBECONFIG: /home/ubuntu/.kube/config
          # MongoDB resources
          K8S_MONGODB_REPLICAS: ${{ secrets.K8S_MONGODB_REPLICAS || '1' }}
          K8S_MONGODB_MEMORY_REQUEST: ${{ secrets.K8S_MONGODB_MEMORY_REQUEST || '512Mi' }}
          K8S_MONGODB_MEMORY_LIMIT: ${{ secrets.K8S_MONGODB_MEMORY_LIMIT || '1Gi' }}
          K8S_MONGODB_CPU_REQUEST: ${{ secrets.K8S_MONGODB_CPU_REQUEST || '50m' }}
          K8S_MONGODB_CPU_LIMIT: ${{ secrets.K8S_MONGODB_CPU_LIMIT || '200m' }}
          # Redis resources
          K8S_REDIS_REPLICAS: ${{ secrets.K8S_REDIS_REPLICAS || '1' }}
          K8S_REDIS_MEMORY_REQUEST: ${{ secrets.K8S_REDIS_MEMORY_REQUEST || '256Mi' }}
          K8S_REDIS_MEMORY_LIMIT: ${{ secrets.K8S_REDIS_MEMORY_LIMIT || '512Mi' }}
          K8S_REDIS_CPU_REQUEST: ${{ secrets.K8S_REDIS_CPU_REQUEST || '50m' }}
          K8S_REDIS_CPU_LIMIT: ${{ secrets.K8S_REDIS_CPU_LIMIT || '200m' }}
          # PostgreSQL resources
          K8S_POSTGRES_REPLICAS: ${{ secrets.K8S_POSTGRES_REPLICAS || '1' }}
          K8S_POSTGRES_MEMORY_REQUEST: ${{ secrets.K8S_POSTGRES_MEMORY_REQUEST || '128Mi' }}
          K8S_POSTGRES_MEMORY_LIMIT: ${{ secrets.K8S_POSTGRES_MEMORY_LIMIT || '256Mi' }}
          K8S_POSTGRES_CPU_REQUEST: ${{ secrets.K8S_POSTGRES_CPU_REQUEST || '50m' }}
          K8S_POSTGRES_CPU_LIMIT: ${{ secrets.K8S_POSTGRES_CPU_LIMIT || '200m' }}
          # Colyseus resources
          K8S_COLYSEUS_REPLICAS: ${{ secrets.K8S_COLYSEUS_REPLICAS || '1' }}
          K8S_COLYSEUS_MEMORY_REQUEST: ${{ secrets.K8S_COLYSEUS_MEMORY_REQUEST || '256Mi' }}
          K8S_COLYSEUS_MEMORY_LIMIT: ${{ secrets.K8S_COLYSEUS_MEMORY_LIMIT || '512Mi' }}
          K8S_COLYSEUS_CPU_REQUEST: ${{ secrets.K8S_COLYSEUS_CPU_REQUEST || '50m' }}
          K8S_COLYSEUS_CPU_LIMIT: ${{ secrets.K8S_COLYSEUS_CPU_LIMIT || '200m' }}
          # Judge0 Server resources
          K8S_JUDGE0_SERVER_REPLICAS: ${{ secrets.K8S_JUDGE0_SERVER_REPLICAS || '1' }}
          K8S_JUDGE0_SERVER_MEMORY_REQUEST: ${{ secrets.K8S_JUDGE0_SERVER_MEMORY_REQUEST || '256Mi' }}
          K8S_JUDGE0_SERVER_MEMORY_LIMIT: ${{ secrets.K8S_JUDGE0_SERVER_MEMORY_LIMIT || '512Mi' }}
          K8S_JUDGE0_SERVER_CPU_REQUEST: ${{ secrets.K8S_JUDGE0_SERVER_CPU_REQUEST || '50m' }}
          K8S_JUDGE0_SERVER_CPU_LIMIT: ${{ secrets.K8S_JUDGE0_SERVER_CPU_LIMIT || '200m' }}
          # Judge0 Worker resources
          K8S_JUDGE0_WORKER_REPLICAS: ${{ secrets.K8S_JUDGE0_WORKER_REPLICAS || '1' }}
          K8S_JUDGE0_WORKER_MEMORY_REQUEST: ${{ secrets.K8S_JUDGE0_WORKER_MEMORY_REQUEST || '512Mi' }}
          K8S_JUDGE0_WORKER_MEMORY_LIMIT: ${{ secrets.K8S_JUDGE0_WORKER_MEMORY_LIMIT || '1Gi' }}
          K8S_JUDGE0_WORKER_CPU_REQUEST: ${{ secrets.K8S_JUDGE0_WORKER_CPU_REQUEST || '50m' }}
          K8S_JUDGE0_WORKER_CPU_LIMIT: ${{ secrets.K8S_JUDGE0_WORKER_CPU_LIMIT || '400m' }}
          # Bots resources
          K8S_BOTS_REPLICAS: ${{ secrets.K8S_BOTS_REPLICAS || '1' }}
          K8S_BOTS_MEMORY_REQUEST: ${{ secrets.K8S_BOTS_MEMORY_REQUEST || '128Mi' }}
          K8S_BOTS_MEMORY_LIMIT: ${{ secrets.K8S_BOTS_MEMORY_LIMIT || '256Mi' }}
          K8S_BOTS_CPU_REQUEST: ${{ secrets.K8S_BOTS_CPU_REQUEST || '50m' }}
          K8S_BOTS_CPU_LIMIT: ${{ secrets.K8S_BOTS_CPU_LIMIT || '200m' }}
        run: |
          set -e
          
          # Apply manifests in order with environment substitution
          cd backend/k8s
          
          # Apply namespace
          k3s kubectl apply -f namespaces/
          
          # Apply ConfigMap
          k3s kubectl apply -f configmaps/
          
          # Apply storage
          k3s kubectl apply -f storage/
          
          # Services will be applied with env substitution below (after extracting VM IP)
          
          # Apply statefulsets with env substitution
          for file in statefulsets/*.yaml; do
            envsubst < "$file" | k3s kubectl apply -f -
          done
          
          # Wait briefly for StatefulSets to start pods
          echo "Waiting for StatefulSets to initialize..."
          sleep 10
          
          # Apply PodDisruptionBudgets before deployments
          k3s kubectl apply -f pdbs/
          
          # Get VM IP from COLYSEUS_HOST_IP variable
          export ORACLE_VM_IP="${{ vars.COLYSEUS_HOST_IP }}"
          
          if [ -z "$ORACLE_VM_IP" ]; then
            echo "âš ï¸  Warning: COLYSEUS_HOST_IP not set. Services will use ClusterIP only."
            echo "   Set COLYSEUS_HOST_IP variable with your Oracle VM IP address"
          else
            echo "âœ“ Using Oracle VM IP: $ORACLE_VM_IP"
            export ORACLE_VM_IP
          fi
          
          # Apply services with env substitution (for externalIPs)
          echo "Applying services..."
          for file in services/*.yaml; do
            if [ -f "$file" ]; then
              envsubst < "$file" | k3s kubectl apply -f -
            fi
          done
          
          # Apply MongoDB service if it exists
          if [ -f mongodb/service.yaml ]; then
            envsubst < mongodb/service.yaml | k3s kubectl apply -f -
          fi
          
          # Apply deployments with env substitution
          for file in deployments/*.yaml; do
            envsubst < "$file" | k3s kubectl apply -f -
          done
          
          echo "Deployments applied. Services will start in background."
          
          echo "All resources applied successfully"

      - name: Check pod status before rollout
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Current Pod Status ==="
          k3s kubectl get pods -n codeclashers
          echo ""
          echo "=== Pod Events (last 20) ==="
          k3s kubectl get events -n codeclashers --sort-by='.lastTimestamp' | tail -20
          echo ""
          echo "=== Node Status ==="
          k3s kubectl describe nodes | grep -A 5 "Allocatable:"
          echo ""
          echo "=== Unscheduleable Pods (if any) ==="
          k3s kubectl get pods -n codeclashers -o wide --field-selector=status.phase=Pending
          k3s kubectl describe pod -n codeclashers mongodb-0 | grep -A 5 Events || true

      - name: Wait for rollouts to complete
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "Waiting for deployments to start..."
          
          # Quick non-blocking check - just wait for pods to exist
          sleep 15
          
          echo "Checking deployment status..."
          k3s kubectl get pods -n codeclashers
          
          echo "Deployments applied. Pods will continue starting in background."

      - name: Verify deployments
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Deployment Status ==="
          k3s kubectl get pods -n codeclashers
          echo ""
          echo "=== Services ==="
          k3s kubectl get svc -n codeclashers
          
      - name: Debug failed pods
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Debugging failed pods ==="
          
          # Check Judge0 server logs if it exists
          if k3s kubectl get deployment judge0-server -n codeclashers 2>/dev/null | grep -q judge0-server; then
            echo ""
            echo "--- Judge0 Server Logs ---"
            k3s kubectl logs -n codeclashers deployment/judge0-server --tail=50 || echo "No logs available yet"
          fi
          
          # Check Judge0 worker logs if it exists
          if k3s kubectl get deployment judge0-worker -n codeclashers 2>/dev/null | grep -q judge0-worker; then
            echo ""
            echo "--- Judge0 Worker Logs ---"
            k3s kubectl logs -n codeclashers deployment/judge0-worker --tail=50 || echo "No logs available yet"
          fi
          
          # Check MongoDB logs if it's having issues
          if k3s kubectl get pod mongodb-0 -n codeclashers 2>/dev/null | grep -v "1/1.*Running"; then
            echo ""
            echo "--- MongoDB Logs ---"
            k3s kubectl logs -n codeclashers mongodb-0 --tail=50 || echo "No logs available yet"
          fi
          
      - name: Pre-deployment validation
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Pre-deployment Validation ==="
          
          # Check if required secrets exist (check each one individually)
          echo "Checking required secrets..."
          MISSING_SECRETS=()
          
          # Check each secret individually
          if [ -z "${{ secrets.REDIS_PASSWORD }}" ]; then
            MISSING_SECRETS+=("REDIS_PASSWORD")
          fi
          if [ -z "${{ secrets.MONGODB_URI }}" ]; then
            MISSING_SECRETS+=("MONGODB_URI")
          fi
          if [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            MISSING_SECRETS+=("OPENAI_API_KEY")
          fi
          if [ -z "${{ secrets.INTERNAL_SERVICE_SECRET }}" ]; then
            MISSING_SECRETS+=("INTERNAL_SERVICE_SECRET")
          fi
          if [ -z "${{ secrets.BOT_SERVICE_SECRET }}" ]; then
            MISSING_SECRETS+=("BOT_SERVICE_SECRET")
          fi
          
          if [ ${#MISSING_SECRETS[@]} -gt 0 ]; then
            echo "âŒ Missing required secrets: ${MISSING_SECRETS[*]}"
            exit 1
          fi
          
          echo "âœ“ All required secrets present"
          
          # Validate image tags
          if [ -z "$IMAGE_TAG" ]; then
            echo "âŒ IMAGE_TAG is not set"
            exit 1
          fi
          
          echo "âœ“ Image tag: $IMAGE_TAG"
          
          # Check cluster connectivity
          if ! k3s kubectl cluster-info &>/dev/null; then
            echo "âŒ Cannot connect to Kubernetes cluster"
            exit 1
          fi
          
          echo "âœ“ Cluster connectivity verified"
          
      - name: Health check with rollback
        id: health_check
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Performing Enhanced Health Checks ==="
          
          HEALTH_CHECK_FAILED=false
          
          # Function to check HTTP endpoint
          check_http_endpoint() {
            local service=$1
            local port=$2
            local path=${3:-/}
            local max_attempts=${4:-30}
            
            echo "Checking $service HTTP endpoint (port $port, path $path)..."
            for i in $(seq 1 $max_attempts); do
              # Try to exec into pod and curl the endpoint
              if k3s kubectl exec -n codeclashers deployment/$service -- sh -c "curl -sf http://localhost:$port$path > /dev/null 2>&1" 2>/dev/null; then
                echo "âœ“ $service HTTP endpoint is healthy"
                return 0
              fi
              if [ $i -lt $max_attempts ]; then
                sleep 2
              fi
            done
            echo "âŒ $service HTTP endpoint health check failed"
            return 1
          }
          
          # Function to check port connectivity
          check_port() {
            local service=$1
            local port=$2
            local max_attempts=${3:-30}
            
            echo "Checking $service port $port connectivity..."
            for i in $(seq 1 $max_attempts); do
              if k3s kubectl exec -n codeclashers deployment/$service -- sh -c "nc -z localhost $port" 2>/dev/null; then
                echo "âœ“ $service port $port is open"
                return 0
              fi
              if [ $i -lt $max_attempts ]; then
                sleep 2
              fi
            done
            echo "âŒ $service port $port connectivity check failed"
            return 1
          }
          
          # Wait for deployments to be ready
          echo "Waiting for deployments to be ready..."
          k3s kubectl wait --for=condition=available --timeout=300s deployment/colyseus -n codeclashers || HEALTH_CHECK_FAILED=true
          k3s kubectl wait --for=condition=available --timeout=300s deployment/bots -n codeclashers || HEALTH_CHECK_FAILED=true
          
          if [ "$HEALTH_CHECK_FAILED" = true ]; then
            echo "âŒ Deployments failed to become available"
            echo "false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check Colyseus port connectivity
          if ! check_port "colyseus" "2567" 30; then
            HEALTH_CHECK_FAILED=true
          fi
          
          # Check Colyseus HTTP endpoint (if available)
          # Note: Colyseus may not have a /health endpoint, so we check port only
          
          # Check Judge0 server if it exists (non-blocking - just verify pod is running)
          if k3s kubectl get deployment judge0-server -n codeclashers &>/dev/null; then
            echo "Checking Judge0 server status..."
            if k3s kubectl wait --for=condition=available --timeout=60s deployment/judge0-server -n codeclashers 2>/dev/null; then
              echo "âœ“ Judge0 server deployment is available"
              # Try HTTP check but don't fail if it doesn't work (Judge0 might need more time)
              if check_http_endpoint "judge0-server" "2358" "/" 10 2>/dev/null; then
                echo "âœ“ Judge0 server HTTP endpoint is responding"
              else
                echo "âš ï¸  Judge0 server HTTP endpoint check failed (non-critical - pod is running)"
              fi
            else
              echo "âš ï¸  Judge0 server deployment not ready yet (non-critical)"
            fi
          fi
          
          # Check MongoDB if accessible
          if k3s kubectl get statefulset mongodb -n codeclashers &>/dev/null; then
            echo "Checking MongoDB connectivity..."
            for i in {1..30}; do
              if k3s kubectl exec -n codeclashers mongodb-0 -- mongosh --quiet --eval "db.adminCommand('ping')" 2>/dev/null | grep -q "ok.*1"; then
                echo "âœ“ MongoDB is healthy"
                break
              fi
              if [ $i -eq 30 ]; then
                echo "âš ï¸  MongoDB health check timeout (may still be initializing)"
              else
                sleep 2
              fi
            done
          fi
          
          if [ "$HEALTH_CHECK_FAILED" = true ]; then
            echo "âŒ Health checks failed"
            echo "false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "âœ“ All health checks passed"
          echo "true" >> $GITHUB_OUTPUT
          
      - name: Rollback on health check failure
        if: steps.health_check.outputs.result == 'false'
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "=== Rolling back deployment due to health check failure ==="
          
          # Get previous ReplicaSet
          PREVIOUS_RS=$(k3s kubectl get replicaset -n codeclashers -l app=colyseus --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-2].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$PREVIOUS_RS" ]; then
            echo "Rolling back Colyseus to previous ReplicaSet: $PREVIOUS_RS"
            k3s kubectl rollout undo deployment/colyseus -n codeclashers || true
          else
            echo "No previous ReplicaSet found for Colyseus"
          fi
          
          PREVIOUS_RS=$(k3s kubectl get replicaset -n codeclashers -l app=bots --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-2].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$PREVIOUS_RS" ]; then
            echo "Rolling back Bots to previous ReplicaSet: $PREVIOUS_RS"
            k3s kubectl rollout undo deployment/bots -n codeclashers || true
          else
            echo "No previous ReplicaSet found for Bots"
          fi
          
          echo "=== Rollback complete ==="
          echo "Please check pod logs and events for details:"
          k3s kubectl get pods -n codeclashers
          k3s kubectl get events -n codeclashers --sort-by='.lastTimestamp' | tail -20
          
          exit 1
          
      - name: Deployment complete
        if: steps.health_check.outputs.result == 'true'
        env:
          KUBECONFIG: /home/ubuntu/.kube/config
        run: |
          echo "ðŸš€ Deployment complete!"
          echo "Image tag: $IMAGE_TAG"
          echo ""
          echo "=== Service Status ==="
          k3s kubectl get svc -n codeclashers
          echo ""
          echo "âœ“ Services are bound to Oracle VM IP with predefined ports:"
          echo "   - Colyseus: Port 2567"
          echo "   - Redis: Port 6379"
          echo "   - MongoDB: Port 27017"
          echo "   - Judge0: Port 2358"
          echo ""
          echo "âœ“ No variable updates needed - using predefined IPs and ports from GitHub Variables"
